# üìä An√°lisis Completo del Sistema AcepNet

## üéØ Prop√≥sito General

**AcepNet** es un sistema de aprendizaje autom√°tico (deep learning) que resuelve el problema de **clasificaci√≥n de cadenas en Aut√≥matas Finitos Deterministas (AFDs)**. El modelo aprende a predecir si una cadena es aceptada o rechazada por un AFD espec√≠fico sin necesidad de simular el aut√≥mata directamente.

---

## üèóÔ∏è Arquitectura del Sistema

### 1. **Componentes Principales**

```
acepnet/
‚îú‚îÄ‚îÄ acepten.py              # Arquitectura del modelo y clases base
‚îú‚îÄ‚îÄ inferencia_mejorada.py  # Script de inferencia interactiva
‚îú‚îÄ‚îÄ best_model.pt          # Modelo entrenado (7.3MB)
‚îú‚îÄ‚îÄ dataset6000.csv        # Dataset con 6000 AFDs (9.6MB)
‚îú‚îÄ‚îÄ thresholds.json        # Umbrales calibrados (Y1=0.43, Y2=0.53)
‚îî‚îÄ‚îÄ README.md             # Documentaci√≥n
```

---

## üìö Dataset (`dataset6000.csv`)

### Estructura de Datos

El dataset contiene **6000 AFDs** con las siguientes columnas:

- **Regex**: Expresi√≥n regular original (ej: `[LCIG]+`, `[GDIK]*`)
- **Alfabeto**: S√≠mbolos del alfabeto separados por espacio (ej: `C G I L`)
- **Estados de aceptaci√≥n**: Estados finales (ej: `S3 S4 S2 S1`)
- **Estados**: Todos los estados del AFD (ej: `S0 S1 S2 S3 S4`)
- **Transiciones**: Transiciones en formato `S0 --A--> S1 | S1 --B--> S2`
- **Clase**: Diccionario JSON con 100 cadenas (50 aceptadas, 50 rechazadas) y su valor booleano
- **Error**: Campo para errores (vac√≠o si no hay error)

### Ejemplo de Entrada:

```csv
Regex,Alfabeto,Estados de aceptaci√≥n,Estados,Transiciones,Clase,Error
[LCIG]+,C G I L,S3 S4 S2 S1,S0 S1 S2 S3 S4,S3 --C--> S4 | S3 --G--> S1 | ...,"{'C': true, 'CC': true, ...}", 
```

---

## üß† Arquitectura del Modelo (`DualEncoderModel`)

### Visi√≥n General

El modelo implementa una arquitectura **Dual-Encoder** con **dos tareas simult√°neas**:

1. **Y1**: Predicci√≥n de pertenencia (¬øla cadena pertenece al AFD espec√≠fico?)
2. **Y2**: Predicci√≥n de cadena compartida (¬øla cadena es aceptada por m√∫ltiples AFDs?)

### Componentes del Modelo

#### 1. **String Encoder** (Encoder de Cadenas)

```
Cadena ‚Üí Embedding ‚Üí BiGRU ‚Üí h_str
```

- **Embedding Layer**: Convierte √≠ndices de s√≠mbolos a vectores densos
  - Vocabulario: 12 s√≠mbolos (A-L) + 1 token de padding = 13 tokens
  - Dimensi√≥n de embedding: 32
  
- **BiGRU** (Bidirectional GRU):
  - 2 capas
  - Hidden dimension: 64
  - Bidireccional: output dimension = 64 √ó 2 = **128**
  - Dropout: 0.2 entre capas
  
**Manejo especial**: Las cadenas vac√≠as (√©psilon) se representan como un vector de ceros.

#### 2. **AFD Encoder** (Encoder de Aut√≥matas)

```
Features del AFD ‚Üí MLP ‚Üí h_afd
```

**Representaci√≥n del AFD**:
- **Matriz de transiciones one-hot**: `[16 estados √ó 12 s√≠mbolos √ó 16 estados]` = 3072 features
- **Vector de estados de aceptaci√≥n**: 16 features (uno por estado)
- **Vector de estados v√°lidos**: 16 features (m√°scara de estados existentes)
- **Total**: 3072 + 16 + 16 = **3104 features**

**MLP del AFD Encoder**:
```
3104 ‚Üí Linear(512) ‚Üí ReLU ‚Üí Dropout(0.3)
     ‚Üí Linear(256) ‚Üí ReLU ‚Üí Dropout(0.3)
     ‚Üí Linear(128) ‚Üí ReLU
     ‚Üí h_afd (128 dimensiones)
```

#### 3. **Head 1: Pertenencia (Y1)**

**Input**: Concatenaci√≥n de `h_str` + `h_afd` = 128 + 128 = **256 dimensiones**

```
256 ‚Üí Linear(128) ‚Üí ReLU ‚Üí Dropout(0.3)
    ‚Üí Linear(64) ‚Üí ReLU
    ‚Üí Linear(1) ‚Üí Sigmoid
    ‚Üí y1_hat (probabilidad)
```

#### 4. **Head 2: Cadena Compartida (Y2)**

**Input**: Solo `h_str` = **128 dimensiones**

```
128 ‚Üí Linear(64) ‚Üí ReLU ‚Üí Dropout(0.2)
    ‚Üí Linear(32) ‚Üí ReLU
    ‚Üí Linear(1) ‚Üí Sigmoid
    ‚Üí y2_hat (probabilidad)
```

### Par√°metros del Modelo

- **Total de par√°metros**: ~500,000-600,000 (estimado)
- **Tama√±o del modelo**: 7.3MB (best_model.pt)

---

## üîÑ Pipeline de Entrenamiento (`acepten.py`)

### 1. **Parsing de AFDs** (`AFDParser`)

- **Funci√≥n**: Extrae informaci√≥n estructurada de los AFDs del CSV
- **M√©todos clave**:
  - `parse_states()`: Parsea estados (ej: `"S0 S1 S2"` ‚Üí `[0, 1, 2]`)
  - `parse_accept_states()`: Parsea estados de aceptaci√≥n
  - `parse_transitions()`: Parsea transiciones usando regex
  - `get_afd_features()`: Convierte AFD a vector de 3104 features
  - `simulate_afd()`: Simula el AFD para verificar aceptaci√≥n (ground truth)

### 2. **Generaci√≥n de Dataset** (`StringDatasetGenerator`)

- **Funci√≥n**: Genera pares `(dfa_id, string, label)` para entrenamiento

**Muestras Positivas**:
- Extrae cadenas aceptadas de la columna `Clase` del CSV
- Por defecto: 50 cadenas aceptadas por AFD

**Muestras Negativas**:
- Genera cadenas aleatorias usando el alfabeto del AFD
- Verifica que NO sean aceptadas (simulaci√≥n del AFD)
- Por defecto: 50 cadenas rechazadas por AFD

**C√°lculo de Y2**:
- Cuenta cu√°ntos AFDs distintos aceptan cada cadena
- Si una cadena es aceptada por ‚â•2 AFDs ‚Üí `y2=1`, sino `y2=0`

### 3. **Dataset de PyTorch** (`AFDStringDataset`)

- **Funci√≥n**: Dataset personalizado para PyTorch
- **Tokenizaci√≥n**:
  - Mapeo: `'A' ‚Üí 0, 'B' ‚Üí 1, ..., 'L' ‚Üí 11`
  - Padding: `PAD_IDX = 12`
  - Cadenas vac√≠as: `[]`

### 4. **Collate Function**

- **Funci√≥n**: Maneja secuencias de longitud variable en batches
- **Padding**: Rellena cadenas m√°s cortas con `PAD_IDX` hasta la longitud m√°xima del batch

### 5. **Entrenamiento** (`Trainer`)

**Loss Function**:
```
Loss = Œª1 * BCE(y1_hat, y1_true) + Œª2 * BCE(y2_hat, y2_true)
```
- Por defecto: `Œª1 = 1.0`, `Œª2 = 1.0`

**Optimizador**:
- Adam con learning rate: 0.001
- Weight decay: 1e-5
- Gradiente clipping: max_norm = 5.0

**Scheduler**:
- ReduceLROnPlateau (reduce LR si no mejora en 3 √©pocas, factor=0.5)

**Divisi√≥n del Dataset**:
- Por **dfa_id** (no por ejemplo individual)
- Train: 70% de AFDs
- Val: 15% de AFDs
- Test: 15% de AFDs

**M√©tricas**:
- Accuracy para Y1 y Y2
- Loss promedio

---

## üéØ Inferencia (`inferencia_mejorada.py`)

### Clase `Predictor`

**Funcionalidad**:
- Carga el modelo entrenado (`best_model.pt`)
- Carga umbrales calibrados (`thresholds.json`)
- Realiza predicciones para pares `(dfa_id, string)`

**Validaci√≥n de Alfabeto**:
- Si la cadena contiene s√≠mbolos fuera del alfabeto del AFD:
  - `y1_prob = 0.0` (rechazo autom√°tico)
  - `y2_prob = 0.0`
  - No se ejecuta el modelo

**Umbrales Calibrados**:
- **Y1**: 0.43 (en lugar de 0.5 est√°ndar)
- **Y2**: 0.53 (en lugar de 0.5 est√°ndar)
- Estos umbrales fueron optimizados para mejor precisi√≥n

### Modo Interactivo

El script ofrece 5 opciones:

1. **Probar cadena con AFD (por ID)**: Ingresa ID y cadena, muestra predicci√≥n vs ground truth
2. **Buscar AFD por palabra clave**: Busca AFDs cuya regex contenga una palabra
3. **Ver informaci√≥n de AFD**: Muestra detalles sin predicci√≥n
4. **Ejemplos predefinidos**: Ejecuta casos de prueba autom√°ticos
5. **Salir**: Cierra el programa

---

## üìä M√©tricas y Evaluaci√≥n

### M√©tricas Principales

**Para Y1 (Pertenencia)**:
- **Accuracy**: Porcentaje de predicciones correctas
- **F1 Score**: Balance entre precisi√≥n y recall
- **Clasificaci√≥n de rendimiento**:
  - ‚úÖ MUY BUENO: Accuracy ‚â• 0.95 y F1 ‚â• 0.95
  - ‚úîÔ∏è BUENO: Accuracy ‚â• 0.90 y F1 ‚â• 0.90
  - ‚ö†Ô∏è REGULAR: Accuracy ‚â• 0.85
  - ‚ùå MALO: Accuracy < 0.85

**Para Y2 (Cadena Compartida)**:
- **Accuracy**: Porcentaje de predicciones correctas
- **F1 Score**: Balance entre precisi√≥n y recall
- **PR-AUC** (Precision-Recall Area Under Curve): √Årea bajo la curva PR

---

## üîß Caracter√≠sticas T√©cnicas

### 1. **Constantes Globales**

```python
ALPHABET = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L']
MAX_STATES = 16
NUM_SYMBOLS = 12
PAD_IDX = 12
```

### 2. **Manejo de Cadenas Vac√≠as (√âpsilon)**

- Las cadenas vac√≠as se representan como `[]` (lista vac√≠a)
- En el modelo, se manejan especialmente: se produce un vector de ceros para `h_str`
- Permite al modelo aprender que algunas cadenas vac√≠as son aceptadas

### 3. **Cache de Features de AFD**

- `AFDParser` mantiene un cache (`afd_cache`) para evitar recalcular features
- Las features se calculan una vez por AFD y se reutilizan

### 4. **Padding Din√°mico**

- El padding se hace por batch (no global)
- Cada batch solo paddea hasta la longitud m√°xima dentro de ese batch
- M√°s eficiente en memoria

---

## üéì Ventajas del Sistema

1. **Aprendizaje Autom√°tico**: No requiere simular el AFD para predecir aceptaci√≥n
2. **Velocidad**: Predicciones r√°pidas una vez entrenado el modelo
3. **Escalabilidad**: Puede generalizar a AFDs no vistos durante entrenamiento
4. **Multi-tarea**: Aprende dos tareas relacionadas simult√°neamente (Y1 y Y2)
5. **Validaci√≥n Inteligente**: Rechaza autom√°ticamente cadenas con s√≠mbolos inv√°lidos

---

## ‚ö†Ô∏è Limitaciones

1. **Alfabeto Fijo**: Solo funciona con s√≠mbolos A-L (12 s√≠mbolos)
2. **Estados M√°ximos**: Limitado a 16 estados por AFD
3. **Dependencia del Dataset**: Requiere un dataset de AFDs pre-generado
4. **Umbrales Calibrados**: Los umbrales fueron ajustados para este dataset espec√≠fico
5. **Modelo Espec√≠fico**: El modelo est√° entrenado para este dominio espec√≠fico

---

## üîÑ Flujo de Datos Completo

```
1. Dataset CSV (6000 AFDs)
   ‚Üì
2. AFDParser ‚Üí Extrae features (3104 dim)
   ‚Üì
3. StringDatasetGenerator ‚Üí Genera pares (dfa_id, string, y1, y2)
   ‚Üì
4. AFDStringDataset ‚Üí Tokeniza cadenas
   ‚Üì
5. DataLoader ‚Üí Batching con padding
   ‚Üì
6. DualEncoderModel:
   - String ‚Üí Embedding ‚Üí BiGRU ‚Üí h_str (128)
   - AFD ‚Üí MLP ‚Üí h_afd (128)
   ‚Üì
7. Head 1: concat(h_str, h_afd) ‚Üí Y1 (pertenencia)
8. Head 2: h_str ‚Üí Y2 (compartida)
   ‚Üì
9. Loss ‚Üí Backpropagation ‚Üí Optimizaci√≥n
```

---

## üöÄ Casos de Uso

1. **Clasificaci√≥n R√°pida**: Determinar si una cadena pertenece a un AFD sin simular
2. **B√∫squeda de Patrones**: Identificar cadenas compartidas entre m√∫ltiples AFDs
3. **Validaci√≥n de Alfabetos**: Detectar autom√°ticamente s√≠mbolos inv√°lidos
4. **Investigaci√≥n**: Estudiar la capacidad de las redes neuronales para aprender lenguajes regulares

---

## üìà Mejoras Futuras Potenciales

1. **Alfabeto Din√°mico**: Soporte para alfabetos de cualquier tama√±o
2. **M√°s Estados**: Aumentar el l√≠mite de estados (actualmente 16)
3. **Atenci√≥n**: Agregar mecanismos de atenci√≥n para mejor interpretabilidad
4. **Transfer Learning**: Pre-entrenar en un dataset m√°s grande y fine-tune en dominios espec√≠ficos
5. **Interpretabilidad**: Visualizar qu√© partes de la cadena y del AFD son m√°s importantes para la decisi√≥n

---

## üîç Resumen Ejecutivo

**AcepNet** es un sistema de deep learning que aprende a clasificar cadenas en AFDs usando una arquitectura dual-encoder. El modelo procesa tanto la cadena de entrada (usando BiGRU) como la representaci√≥n del AFD (usando MLP), y produce dos predicciones: pertenencia a un AFD espec√≠fico (Y1) y si la cadena es compartida entre m√∫ltiples AFDs (Y2). El sistema est√° entrenado en 6000 AFDs y puede hacer predicciones r√°pidas sin necesidad de simular el aut√≥mata.

